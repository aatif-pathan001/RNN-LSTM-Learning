{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Character-Level RNN for Text Generation**\n",
    "\n",
    "Learning objective: Understand RNN architecture and training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the architecture of **Multilayer RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multilayer RNN](image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMQvQtW5T_AZ",
    "outputId": "664e8682-62f8-4bb9-eab1-2a0b80f36863"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wz1ZF1CJN2gx",
    "outputId": "571dcad7-03a4-4680-ca99-81f383ed63f3"
   },
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU is available. Using CUDA.')\n",
    "    print('GPU Name:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('GPU not found. Using CPU.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz_dO77-UIOq"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 0.0003\n",
    "NUM_EPOCHS = 200\n",
    "SEQ_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4PLerK0UIxy"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN for character-level text generation\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # RNN layer: output only the hidden state (h), calculate. It doesn't apply the softmax function, nor does the hidden x output layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        x: (batch, seq_len, input_size)\n",
    "        hidden: (num_layers, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        # out: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Reshape for FC layer\n",
    "        out = out.reshape(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiuvYdJ7USBt"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for character-level text\"\"\"\n",
    "    def __init__(self, text, seq_length):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.chars = sorted(list(set(text)))  # extracting characters only (A-Z).\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}  # creating a dictionary/ vocabolary out of it.\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns input sequence and target (next character)\n",
    "        \"\"\"\n",
    "        seq = self.text[idx:idx+self.seq_length]\n",
    "        target = self.text[idx+1:idx+self.seq_length+1]\n",
    "\n",
    "        # Convert to indices\n",
    "        seq_idx = [self.char_to_idx[ch] for ch in seq]\n",
    "        target_idx = [self.char_to_idx[ch] for ch in target]\n",
    "\n",
    "        return torch.tensor(seq_idx), torch.tensor(target_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqMN1zWJUhrw"
   },
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "UJImb-GIUTtg",
    "outputId": "b97f5384-946e-4991-f3ea-c70d8adce6e8"
   },
   "outputs": [],
   "source": [
    "dataset = TextDataset(text, SEQ_LENGTH)  # return pair of  (in_seq, target_seq).\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last = True, num_workers= 2, pin_memory=True)  #  returns 32 {batch_size} x pairs of (in,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-V-WmhuUVt2"
   },
   "outputs": [],
   "source": [
    "model = CharRNN(\n",
    "        input_size=dataset.vocab_size,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        output_size=dataset.vocab_size\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKYsgiKCUq68"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XRVK8hvoTDR"
   },
   "outputs": [],
   "source": [
    "#op_model = torch.load('/content/char_rnn_model.pth')\n",
    "#model.load_state_dict(op_model)\n",
    "# If you also want to load the optimizer state, you would need to save it along with the model state\n",
    "# optimizer.load_state_dict(op_model['optimizer_state_dict']) # This line will still cause an error with current saving method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSQYpid5UrSz"
   },
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (sequences, targets) in enumerate(dataloader):\n",
    "        current_batch_size = sequences.shape[0]\n",
    "        # Initialize hidden state for the current batch and move to device\n",
    "        hidden = model.init_hidden(current_batch_size).to(device)\n",
    "\n",
    "        # Move sequences and targets to the device\n",
    "        sequences = sequences.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        # One-hot encode input\n",
    "        sequences_onehot = torch.nn.functional.one_hot(\n",
    "            sequences,\n",
    "            num_classes=dataset.vocab_size\n",
    "        ).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden = model(sequences_onehot, hidden.detach())\n",
    "        loss = criterion(outputs, targets.view(-1))\n",
    "        loss_arr.append(loss.item())\n",
    "\n",
    "        old_weights = {name: param.clone() for name, param in model.named_parameters()}\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #for name, param in model.named_parameters():\n",
    "          #if param.grad is not None:\n",
    "            # Calculate the L2 norm of the gradient\n",
    "            #grad_norm = param.grad.norm().item()\n",
    "            #print(f\"Layer: {name} | Gradient Norm: {grad_norm:.8f}\")\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "          weight_change = (param - old_weights[name]).norm().item()\n",
    "          if weight_change == 0:\n",
    "            print(f\"ALERT: {name} weights did NOT change!\")\n",
    "          else:\n",
    "            print(f\"{name} updated by {weight_change:.8f}\")\n",
    "\n",
    "        # Generate sample\n",
    "        #sample = generate_text(model, dataset, start_str=\"The \", length=200)\n",
    "        #print(f'Generated: {sample}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoAQc1VxUz8D"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'char_rnn_model.pth')\n",
    "plt.plot(loss_arr, 'r-')\n",
    "plt.show()\n",
    "print('Loss before training', loss_arr[0])\n",
    "print('Loss after training', loss_arr[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmTxwKzPVtvS"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, dataset, start_str=\"The \", length=100):\n",
    "    \"\"\"Generate text using trained model\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    chars = [ch for ch in start_str]\n",
    "    hidden = model.init_hidden(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "            # Encode current sequence\n",
    "            seq_idx = [dataset.char_to_idx[ch] for ch in chars[-SEQ_LENGTH:]]\n",
    "            seq_tensor = torch.tensor([seq_idx]).to(device)\n",
    "            seq_onehot = torch.nn.functional.one_hot(\n",
    "                seq_tensor,\n",
    "                num_classes=dataset.vocab_size\n",
    "            ).float()\n",
    "\n",
    "            # Predict next character\n",
    "            output, hidden = model(seq_onehot, hidden)\n",
    "\n",
    "            # Sample from output distribution\n",
    "            prob = torch.nn.functional.softmax(output[-1], dim=0)\n",
    "            next_char_idx = torch.multinomial(prob, 1).item()\n",
    "            next_char = dataset.idx_to_char[next_char_idx]\n",
    "\n",
    "            chars.append(next_char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4rM5OrQOHTs",
    "outputId": "ec9cd52c-ca6e-48ae-cce7-36c418147009"
   },
   "outputs": [],
   "source": [
    "# Generate sample\n",
    "sample = generate_text(model, dataset, start_str=\"the\", length=30)\n",
    "print(f'Generated: {sample}\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
