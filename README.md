# RNN & LSTM Deep Learning Journey

## Overview
This repository documents my learning journey with Recurrent Neural Networks and LSTMs, from theory to implementation.

## Projects

### 1. Simple Character-Level RNN
- **Objective**: Understand basic RNN architecture
- **Dataset**: Text corpus (Shakespeare/books)
- **Results**: [Loss curves, generated samples]
- **Code**: `simple_rnn.py`

### 2. LSTM Text Generator
- **Objective**: Compare RNN vs LSTM
- **Improvements**: Better long-term dependencies
- **Code**: `lstm_implementation.py`

### 3. Sentiment Analysis (LSTM)
- **Dataset**: IMDB or Twitter sentiment
- **Accuracy**: [Your results]
- **Code**: `sentiment_analysis.py`

### 4. Stock Price Predictor
- **Architecture**: Multi-layer LSTM
- **Features**: Technical indicators
- **Demo**: [Link to Gradio/Streamlit app]
- **Code**: `stock_price_predictor/`

## What I Learned

### Technical Concepts:
- Sequential data processing
- Hidden state and cell state
- Vanishing gradient problem
- LSTM gates (forget, input, output)

### Implementation Skills:
- PyTorch RNN/LSTM modules
- Sequence data preprocessing
- Training loops for sequential models
- Text generation sampling

### Challenges Overcome:
1. [Challenge 1]: [How you solved it]
2. [Challenge 2]: [Solution]

## Resources Used
- [Andrej Karpathy's Blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [colah's Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- PyTorch Documentation

## Next Steps
- [ ] Implement GRU and compare with LSTM
- [ ] Build bidirectional LSTM
- [ ] Try attention mechanisms

---

**Author**: Aatif Khan Pathan
**Duration**: Week 1 of ML Learning Journey
